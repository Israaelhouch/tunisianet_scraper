# Web Scraping & Automation Project

## Project Overview
This Python-based project is designed for **web scraping and automation**. It extracts data from websites dynamically and efficiently, handling both static and JavaScript-rendered pages.

---

## Features
- Extract categories, subcategories, and item URLs from websites.
- Handle multi-page navigation and hover-based content.
- Supports static HTML parsing (**BeautifulSoup**) and dynamic scraping (**Selenium**).
- Saves data in structured formats: CSV or JSON.
- Implements logging and error handling for robust scraping.

---

## Technologies & Libraries
- **Python 3.x**
- **Selenium** â€“ Browser automation for dynamic content.
- **BeautifulSoup** â€“ HTML parsing.
- **WebDriverWait & ExpectedConditions** â€“ Manage dynamic elements.
- **pandas** â€“ Data storage and manipulation.
- **logging** â€“ Track scraping process and errors.
- Optional: **MongoDB/SQL** â€“ For structured data storage.

---

## Installation

```bash
git clone https://github.com/yourusername/project-name.git  # Clone the repository
cd tunisianet_scraper                                       # Navigate to the project folder
pip install -r requirements.txt                             # Install dependencies
python main.py                                              # pip install -r requirements.txt
```

---

## Project Structure

```markdown
project-name/
â”œâ”€â”€ ğŸ“ data
â”‚   â”œâ”€â”€ ğŸ“ __pycache__/ ğŸš« (auto-hidden)
â”‚   â”œâ”€â”€ ğŸ“ utils/
â”‚   â”‚   â”œâ”€â”€ ğŸ“ __pycache__/ ğŸš« (auto-hidden)
â”‚   â”‚   â”œâ”€â”€ ğŸ categories_extraction.py
â”‚   â”‚   â”œâ”€â”€ ğŸ config.py
â”‚   â”‚   â””â”€â”€ ğŸ driver.py
â”‚   â”œâ”€â”€ ğŸ category_parser.py
â”‚   â”œâ”€â”€ ğŸ page_parser.py
â”‚   â””â”€â”€ ğŸ storage.py
â”œâ”€â”€ ğŸš« .gitignore
â”œâ”€â”€ ğŸ“– Readme.md
â”œâ”€â”€ ğŸ main.py
â”œâ”€â”€ ğŸ“„ requirements.txt
â””â”€â”€ ğŸ“‹ scraper.log ğŸš« (auto-hidden)
```